{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, LlamaForCausalLM, DPRQuestionEncoder, RagModel, AutoTokenizer, RagTokenForGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BartTokenizerFast' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m question_encoder \u001b[38;5;241m=\u001b[39m DPRQuestionEncoder\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-question_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m generator_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mRagConfig(\n\u001b[1;32m---> 12\u001b[0m     question_encoder\u001b[38;5;241m.\u001b[39mconfig, generator_tokenizer\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m     13\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     dataset_path\u001b[38;5;241m=\u001b[39mdataset_path,\n\u001b[0;32m     15\u001b[0m     index_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_index.faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RagRetriever\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     20\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BartTokenizerFast' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n",
    "from transformers import RagRetriever\n",
    "\n",
    "dataset_path = \"my_dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n",
    "index_path = \"my_index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n",
    "\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "\n",
    "config = transformers.RagConfig(\n",
    "    question_encoder.config, generator_tokenizer.config,\n",
    "    index_name=\"custom\",\n",
    "    dataset_path=dataset_path,\n",
    "    index_path=\"my_index.faiss\")\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type dpr to instantiate a model of type rag. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Config has to be initialized with question_encoder and generator config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# dataset saved via *dataset.save_to_disk(...)*\u001b[39;00m\n\u001b[0;32m      2\u001b[0m index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_index.faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RagRetriever\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     passages_path\u001b[38;5;241m=\u001b[39mdataset_path,\n\u001b[0;32m      7\u001b[0m     index_path\u001b[38;5;241m=\u001b[39mindex_path,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:443\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, retriever_name_or_path, indexed_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    442\u001b[0m     requires_backends(\u001b[38;5;28mcls\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 443\u001b[0m     config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m RagConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    444\u001b[0m     rag_tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m    445\u001b[0m     question_encoder_tokenizer \u001b[38;5;241m=\u001b[39m rag_tokenizer\u001b[38;5;241m.\u001b[39mquestion_encoder\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\configuration_utils.py:609\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    604\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\configuration_utils.py:761\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[1;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 761\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    764\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\rag\\configuration_rag.py:129\u001b[0m, in \u001b[0;36mRagConfig.__init__\u001b[1;34m(self, vocab_size, is_encoder_decoder, prefix, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, title_sep, doc_sep, n_docs, max_combined_length, retrieval_vector_size, retrieval_batch_size, dataset, dataset_split, index_name, index_path, passages_path, use_dummy_dataset, reduce_loss, label_smoothing, do_deduplication, exclude_bos_score, do_marginalize, output_retrieved, use_cache, forced_eos_token_id, dataset_revision, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     87\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    118\u001b[0m         bos_token_id\u001b[38;5;241m=\u001b[39mbos_token_id,\n\u001b[0;32m    119\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m--> 129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m    130\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig has to be initialized with question_encoder and generator config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     question_encoder_config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m     question_encoder_model_type \u001b[38;5;241m=\u001b[39m question_encoder_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Config has to be initialized with question_encoder and generator config"
     ]
    }
   ],
   "source": [
    "dataset_path = \"my_dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n",
    "index_path = \"my_index.faiss\"    # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,\n",
    "    index_path=index_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m RagModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever)\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_config = transformers.RagConfig()\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/rag-token-base\", config=rag_config)\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", question_encoder=question_encoder, tokenizer=tokenizer, dataset_path=\"path/to/my/dataset\", index_name=\"compressed\")\n",
    "\n",
    "rag_model = transformers.RagModel(config=rag_config, generator=model, retriever=retriever, question_encoder=question_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RagTokenizer, RagRetriever, RagSequenceForGeneration\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-sequence-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RagRetriever\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-sequence-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, passages_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_data/my_passages.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m RagSequenceForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-sequence-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever)\n\u001b[0;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:429\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     index \u001b[38;5;241m=\u001b[39m CustomHFIndex(config\u001b[38;5;241m.\u001b[39mretrieval_vector_size, indexed_dataset)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 429\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index(config)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    431\u001b[0m     config,\n\u001b[0;32m    432\u001b[0m     question_encoder_tokenizer\u001b[38;5;241m=\u001b[39mquestion_encoder_tokenizer,\n\u001b[0;32m    433\u001b[0m     generator_tokenizer\u001b[38;5;241m=\u001b[39mgenerator_tokenizer,\n\u001b[0;32m    434\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    435\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:403\u001b[0m, in \u001b[0;36mRagRetriever._build_index\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LegacyIndex(\n\u001b[0;32m    399\u001b[0m         config\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    400\u001b[0m         config\u001b[38;5;241m.\u001b[39mindex_path \u001b[38;5;129;01mor\u001b[39;00m LEGACY_INDEX_PATH,\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mindex_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomHFIndex\u001b[38;5;241m.\u001b[39mload_from_disk(\n\u001b[0;32m    404\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    405\u001b[0m         dataset_path\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpassages_path,\n\u001b[0;32m    406\u001b[0m         index_path\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CanonicalHFIndex(\n\u001b[0;32m    410\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    411\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m         use_dummy_dataset\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_dummy_dataset,\n\u001b[0;32m    416\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:309\u001b[0m, in \u001b[0;36mCustomHFIndex.load_from_disk\u001b[1;34m(cls, vector_size, dataset_path, index_path)\u001b[0m\n\u001b[0;32m    307\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading passages from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m index_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `dataset.get_index(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).save(index_path)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m     )\n\u001b[0;32m    313\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_from_disk(dataset_path)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(vector_size\u001b[38;5;241m=\u001b[39mvector_size, dataset\u001b[38;5;241m=\u001b[39mdataset, index_path\u001b[38;5;241m=\u001b[39mindex_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`."
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", indexed_dataset=\"wiki_dpr\")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n",
    "\n",
    "inputs = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(\"Paris\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model(input_ids=inputs[\"input_ids\"], labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "pytorch_model.bin: 100%|██████████| 2.06G/2.06G [00:59<00:00, 34.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RagRetriever, RagModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n",
    ")\n",
    "# initialize with RagRetriever to do everything in one forward call\n",
    "model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids=inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCa Milà / Barcelona. G building became an exhibition room with an example of modernism in the Eixample. G building is 1,323 m per floor on a plot of 1,620 m. Gaudí made the first sketches in his workshop in the Sagrada Família. He designed the house as a constant curve, both outside and inside, incorporating ruled geometry and naturalistic elements. Casa Milà consists of two buildings, which are structured around two courtyards that provide light to the nine storeys: basement, ground floor, mezzanine, main (or noble) floor, four upper floors, and an attic. The basement was intended to be the garage, the me house many people live in paris? Thep,,,gggibibibibibibibibiberiberiberbumbumiberbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumbumhabibumbumhabibumbumbumbumbumbumhabihabihabihabihabihabihabihabihabihabihabihabihabibumbumbumhabihabibumbum\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "decoded_text = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import DPRContextEncoderTokenizer, DPRContextEncoder\n",
    "\n",
    "class CustomFAISSRetriever:\n",
    "    def __init__(self, index_path, dataset):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        self.encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        # Encode query\n",
    "        inputs = self.tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        query_embedding = self.encoder(**inputs).pooler_output.detach().float().numpy()\n",
    "        \n",
    "        # Search in FAISS\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Fetch documents\n",
    "        return [self.dataset[int(idx)][\"documents\"] for idx in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': 'The quick brown fox jumps over the lazy dog.', 'embeddings': [-0.2997704744338989, 0.008514147251844406, -0.07928809523582458, 0.13623757660388947, 0.04770223796367645, 0.11831056326627731, 0.09454035013914108, 0.011975111439824104, -0.2180950790643692, -0.5996363162994385, -0.24460801482200623, 0.5288651585578918, -0.12938494980335236, -0.4009478688240051, -0.34350448846817017, 0.098673015832901, 0.3680660128593445, 0.3042343854904175, -0.1828700453042984, -0.8829535841941833, -0.06665748357772827, -0.5842864513397217, -0.3403138816356659, -0.29432517290115356, 0.18226534128189087, 0.2895488739013672, 0.20091497898101807, 0.13872678577899933, -0.13819852471351624, 0.08348187059164047, -0.012238632887601852, -0.22817988693714142, 0.21671785414218903, -0.10647594183683395, -0.04387093335390091, -0.09266112744808197, 0.3513854146003723, -0.5129244923591614, -0.21669374406337738, -0.3910924792289734, 0.11717943847179413, 0.44994306564331055, 0.024473942816257477, 0.6001249551773071, 0.020633898675441742, -0.3639105260372162, -2.0025558471679688, 0.04739954695105553, 0.26966118812561035, -0.5922557711601257, -0.20665103197097778, 0.15792499482631683, 0.027539009228348732, 0.6708617806434631, -0.22011318802833557, 0.08512623608112335, -0.5071068406105042, 0.2065925896167755, -0.009534882381558418, -0.062269456684589386, -0.061879709362983704, -0.007736712694168091, 0.5082563161849976, 0.6319154500961304, -0.48805850744247437, 0.9212111830711365, 0.19957983493804932, 0.36100178956985474, 0.5312697291374207, 0.12087812274694443, 0.20977361500263214, -0.3836802542209625, -0.3270183801651001, 0.7129418253898621, -0.14604762196540833, -0.38168099522590637, 0.1787501871585846, -0.19242939352989197, 0.43475911021232605, 0.6758013963699341, 0.016063803806900978, 0.040465887635946274, 0.24149805307388306, 0.35950663685798645, 0.11765450984239578, 0.5001744031906128, -0.16374345123767853, -0.037542473524808884, -0.5295047760009766, 0.3038025498390198, -0.022528551518917084, -0.47834908962249756, -0.29155269265174866, 0.14237210154533386, -0.311165988445282, -0.03390747681260109, 0.7446696162223816, -0.6376621723175049, 0.6817408800125122, 0.1064821258187294, -0.40519291162490845, -0.1316298246383667, -0.12113095819950104, -0.047823674976825714, -0.6499624848365784, -0.4251726269721985, -0.3754459023475647, -0.5608875751495361, -0.34959161281585693, -0.6275759935379028, -0.1205722764134407, 0.013009067624807358, 0.16240601241588593, 0.02795380726456642, -0.17486079037189484, 0.4483497142791748, 0.3268936276435852, 0.08063044399023056, -0.08001159876585007, 0.09193563461303711, -0.08982223272323608, 0.5528966188430786, 0.5055233836174011, 0.08536975830793381, 0.4607928991317749, -0.3832389712333679, 0.1743997037410736, 0.2830020785331726, 0.4000941216945648, -0.10936523228883743, 0.3122541904449463, 0.793817400932312, -0.4760805368423462, -0.5961189270019531, 0.3103678226470947, -0.032708652317523956, 0.5274665951728821, 0.05771072953939438, -0.5046834945678711, -0.0060600340366363525, -0.2968383729457855, 0.07479996234178543, -0.6401838660240173, 0.3040514290332794, 0.5005515813827515, 0.027747437357902527, -0.1822187453508377, -0.41552937030792236, 0.08325397223234177, 0.40196260809898376, 0.011544846929609776, 0.3036412298679352, 0.02268945425748825, 0.635784387588501, -0.6691833734512329, 0.3648863136768341, -0.1990576684474945, -0.35002148151397705, 0.5035639405250549, 0.7702844738960266, -0.181671604514122, -0.2606503367424011, 0.07234577089548111, -0.7612131834030151, 0.30713486671447754, -0.43561428785324097, 0.5728211998939514, -0.26619091629981995, -0.7433927655220032, 0.15948478877544403, -0.3002486526966095, 0.2058148980140686, 0.12910927832126617, -0.5334781408309937, 0.155086487531662, 0.2201927900314331, -0.21406018733978271, -0.21410182118415833, 0.23379145562648773, -0.4968697726726532, -0.7704877257347107, 0.17474012076854706, -0.13213302195072174, -0.3067551255226135, 0.06315792351961136, -0.2513088583946228, -0.061617761850357056, 0.2625446915626526, -0.081214539706707, -0.24140417575836182, -0.11980493366718292, 0.15559370815753937, 0.5147309303283691, -0.575016438961029, 0.88910311460495, -1.076859951019287, -0.29453378915786743, -0.33276230096817017, 0.09734420478343964, -0.05297347158193588, -0.7322241067886353, -0.5055683851242065, 0.2895001769065857, -0.15700916945934296, -0.2620089650154114, 0.9709163308143616, -0.008025601506233215, 0.2652933597564697, 0.4403197765350342, 0.4091326892375946, 0.2523810565471649, 0.43699270486831665, -0.2824488878250122, -0.22066259384155273, -0.04406700283288956, 0.04021449387073517, 0.4271060824394226, 0.15492257475852966, 0.2215598225593567, 0.443851500749588, 0.35391682386398315, 0.07243791222572327, 0.0766766220331192, 0.4967012405395508, -0.04383387789130211, 0.2722851634025574, 0.5871976017951965, 0.28704261779785156, 0.0028469935059547424, -0.7282763719558716, 0.04356861859560013, -0.36042824387550354, 0.03649669140577316, 0.033002130687236786, -0.37776708602905273, -0.6438679099082947, 0.23272259533405304, -0.3226516842842102, 0.7153558731079102, 0.048969946801662445, 0.12989842891693115, 0.1025557741522789, 0.01467413455247879, 0.6054794192314148, -0.32532885670661926, -0.2209610641002655, 0.11517801880836487, 0.6819192171096802, -0.7148736119270325, -0.7835979461669922, 0.23740006983280182, 0.5992460250854492, 0.5761635899543762, -0.9490470886230469, 0.2397727519273758, -0.16799212992191315, -0.26786506175994873, -0.11658693104982376, 0.30683720111846924, 0.4098720848560333, 0.25809332728385925, 0.05412865802645683, -0.27186065912246704, 0.7914812564849854, -0.04036523029208183, -0.3760771155357361, -0.8942551016807556, -0.4943348169326782, -0.12744548916816711, 0.29150328040122986, 0.37725889682769775, 0.4141165614128113, -0.11469054222106934, -1.306722640991211, -0.05688554793596268, 0.02793041244149208, 0.6363662481307983, 0.2399078905582428, -0.006984153296798468, -0.570139467716217, 0.13345091044902802, 0.26585468649864197, 0.0848773866891861, 0.07774164527654648, 0.021763235330581665, -0.12167982012033463, 0.16316132247447968, -0.572845995426178, -0.5493808388710022, 0.033425770699977875, 0.048662327229976654, 0.7275028228759766, -0.09907440096139908, -0.022859998047351837, -0.4655418395996094, 0.015063650906085968, -0.21071085333824158, 0.13595367968082428, 0.4219113886356354, -0.15784239768981934, 0.3161817193031311, 0.751642107963562, -0.07938650250434875, -0.06691916286945343, 0.3841251730918884, -0.4282968044281006, -0.35088950395584106, 0.225830078125, -5.979524612426758, 0.016819283366203308, -0.26180267333984375, 0.02804478071630001, 0.3640047013759613, -0.7589244842529297, 0.22968992590904236, -0.07116334140300751, -0.3704918324947357, 0.8107621669769287, -0.13979493081569672, 3.866851329803467e-06, 0.21541951596736908, 0.023628773167729378, 0.04000309854745865, -0.17661668360233307, -0.12451846152544022, 0.06870798766613007, -0.37554484605789185, 0.2977848947048187, -0.4864371716976166, -0.10681725293397903, -0.3123774230480194, 0.12728601694107056, -0.1069505512714386, -0.028813831508159637, -0.5765400528907776, -0.39649373292922974, -0.12010391056537628, -0.098785899579525, 0.1360200047492981, -0.15089896321296692, 0.23841682076454163, 0.09130607545375824, -0.2616141140460968, 0.03701221942901611, 0.21807906031608582, 0.04887391999363899, -0.05707349255681038, 0.48226603865623474, -0.021225005388259888, 0.4668252170085907, 0.1379503607749939, 0.3220697343349457, 0.2962379455566406, -0.36014413833618164, -0.22977106273174286, -0.6826862692832947, 0.07365124672651291, 0.1484333872795105, -0.25579315423965454, -0.034624069929122925, 0.11120551824569702, 0.015957273542881012, 0.29204413294792175, -0.36557674407958984, 0.16134700179100037, 0.13057802617549896, -0.006344703957438469, -0.2689872682094574, 0.41193753480911255, -0.3739379048347473, 0.24208782613277435, 0.15893705189228058, 0.5294599533081055, -0.3341485857963562, 0.08904790133237839, -0.24200759828090668, 0.389195054769516, 0.12928709387779236, -0.14473891258239746, 0.025508824735879898, 0.24386325478553772, -1.3782894611358643, 0.5874877572059631, -0.10561098158359528, -0.02430661767721176, -0.04672634229063988, -0.03602491319179535, -0.45871701836586, 0.39278939366340637, -0.440606027841568, -0.23657822608947754, 0.16935715079307556, 0.17605653405189514, -0.9411172866821289, -0.01360110193490982, -0.24943111836910248, -0.15500034391880035, 0.22400763630867004, 0.40822672843933105, -0.5401699542999268, -0.5654891133308411, -0.20825640857219696, 0.10195572674274445, 0.6107867956161499, -0.14158272743225098, -0.27867943048477173, -0.42367053031921387, -0.0014561563730239868, 1.4117558002471924, -0.4180748462677002, -0.3884792625904083, -0.24764013290405273, -0.07521674782037735, -0.3330128490924835, -0.008425673469901085, 0.4790383279323578, 0.3575840890407562, -0.1574658453464508, -0.4401320815086365, -0.5460765957832336, -0.2799847722053528, -0.47970032691955566, 0.08646554499864578, 0.0921182706952095, 0.14710503816604614, -0.5453803539276123, 0.18212148547172546, 0.12225248664617538, -0.12779636681079865, 0.20182615518569946, -0.12785345315933228, -0.30974891781806946, 0.01252039521932602, -0.06141141429543495, 0.10659070312976837, -0.12396368384361267, -0.1071346327662468, 0.10563186556100845, -0.10263724625110626, -0.10779401659965515, 0.15270096063613892, -0.5559642910957336, -0.03564915060997009, 0.27250388264656067, 0.03656161576509476, -0.2572290003299713, 0.5377711057662964, -0.34133943915367126, -0.6736554503440857, 0.6655007004737854, 0.021936779841780663, 0.38294288516044617, 0.4705578088760376, -0.04248620942234993, 0.053890712559223175, 0.16639256477355957, -0.5890794992446899, 0.19350410997867584, 0.1417841613292694, -0.05527805536985397, 0.5844541788101196, 0.3991350829601288, -0.2787199020385742, -0.3567553162574768, 0.3705635964870453, 0.035036567598581314, -0.12939387559890747, 0.07844165712594986, 0.18740776181221008, -0.252731055021286, -1.0578250885009766, 0.05229455232620239, 0.4763614535331726, 0.13480333983898163, 0.2770764231681824, -0.8940336108207703, -0.2623172998428345, 0.2947225272655487, -0.33653631806373596, -0.3688177168369293, -0.09759964048862457, 0.31819772720336914, -0.13362187147140503, -0.33041912317276, -0.4371410608291626, 0.20694302022457123, 0.021414682269096375, -0.21122896671295166, 0.2251768857240677, 0.17063982784748077, 0.3588467538356781, 0.6141334176063538, 0.727945864200592, -0.6552141308784485, 0.14245203137397766, -0.05760374665260315, 0.16894745826721191, 0.47605666518211365, 0.5428207516670227, 0.7781316041946411, 0.7147790193557739, 0.7331396341323853, -0.31965193152427673, -0.26352158188819885, 0.04908507317304611, 0.6360717415809631, -0.3066600263118744, -0.34636160731315613, -0.09046341478824615, 0.651037335395813, -0.04879893362522125, 0.10926549881696701, 0.28604400157928467, -0.45103636384010315, 0.3717068135738373, 0.179818257689476, 0.12072042375802994, -0.132121279835701, -0.20258472859859467, -0.6814238429069519, -0.07104374468326569, 0.11209194362163544, -0.023350900039076805, 0.23178575932979584, 0.1249023899435997, -0.5685096383094788, -0.08928360044956207, 0.03408860042691231, 0.27962806820869446, -0.5100709199905396, -0.17247578501701355, 0.07231995463371277, 0.23559829592704773, 0.21737444400787354, -0.13903425633907318, -0.16617414355278015, 0.7268487811088562, 0.040822453796863556, 0.40205103158950806, 0.2549586892127991, 0.10716739296913147, 0.07627195119857788, -0.1085013598203659, -0.00834193080663681, 0.05180729925632477, 0.34775906801223755, 0.3026636242866516, -0.0167598444968462, 0.6555523872375488, 1.013920545578003, 0.14820164442062378, 0.0018655285239219666, -0.055519893765449524, 0.09592203050851822, 0.22109144926071167, -0.4806941747665405, -0.6775210499763489, -0.1307770013809204, 0.5463442206382751, 0.10522506386041641, 0.2916604280471802, 0.04069680720567703, 0.19821593165397644, 0.04552353546023369, -0.6198856830596924, -0.04856215789914131, -0.011666585691273212, 0.2904820442199707, -0.12311045080423355, 0.2891472578048706, -0.36376383900642395, 0.3490685522556305, 0.01711922138929367, 0.24235865473747253, -0.42512866854667664, -0.5245973467826843, 0.01859031617641449, 0.12650105357170105, -0.1463819444179535, -0.9254748225212097, 0.10162484645843506, 0.1595718413591385, -0.03479202464222908, 0.09231805801391602, -0.5696577429771423, 0.11620216816663742, -0.2924906015396118, 0.07633529603481293, 0.5735198259353638, 0.4232128858566284, -0.8720731735229492, -0.008165206760168076, -0.29341381788253784, 0.1915755271911621, 0.34106582403182983, -0.10759933292865753, -0.08037976920604706, 0.46786990761756897, -0.299565851688385, 0.7484801411628723, 0.04309045523405075, -0.32557186484336853, 0.49946239590644836, 0.6739069819450378, 0.7919384837150574, 0.07109277695417404, 0.20377343893051147, 0.010594632476568222, -0.6075882911682129, 0.3796938955783844, 0.38669511675834656, -0.7487215399742126, -0.083585225045681, -0.15110446512699127, 0.10610052198171616, 0.5773922801017761, -0.7098109126091003, 0.3215838372707367, -0.05698268860578537, -0.12680412828922272, -0.27742800116539, 0.2907189130783081, -0.13644255697727203, 0.10335800051689148, -0.03864092007279396, -0.6674507856369019, 0.23591798543930054, 0.2278566211462021, 0.6659542322158813, -0.2335207164287567, 0.24654969573020935, 0.0009273672476410866, -0.6861507892608643, 0.4677450656890869, -0.5155991911888123, 0.9022509455680847, 0.119147390127182, 0.008585162460803986, 0.17286786437034607, 0.1809428185224533, -0.18700826168060303, -0.02753404900431633, 0.4327969253063202, 0.2485477775335312, 0.36591559648513794, -0.32414400577545166, -0.047589261084795, -0.2561401128768921, 0.4176368713378906, 0.01073217298835516, -0.35321447253227234, -0.1871941238641739, -0.27242311835289, 0.12900644540786743, 0.23417635262012482, 0.03400307521224022, -0.05183112993836403, 0.35331085324287415, 0.5162767171859741, 0.3530436158180237, -0.24954664707183838, -0.08961807936429977, -0.12659409642219543, -0.2105523943901062, -0.019713670015335083, -0.25577783584594727, -0.6568230986595154, -0.6816737651824951, 0.40148353576660156, 0.31134775280952454, -0.23462386429309845, 0.03275828808546066, 0.05663708969950676, 0.2962176203727722, 0.004578718915581703, -0.06687047332525253, 0.2220786213874817, 0.24579602479934692, -0.3880751430988312, 0.45101234316825867, 0.04072900116443634, -0.04567968100309372, -0.08241106569766998, -0.3975379467010498, -0.02423591911792755, 0.1938336193561554, -0.17946340143680573, 0.49130311608314514, -0.42784228920936584, 0.6610873341560364, -0.9391246438026428, -0.04262283444404602, -0.2918352782726288, -0.1427803933620453, -0.15552888810634613, -0.5611411333084106, -0.5464948415756226, -0.30749034881591797, -0.01896161586046219, -0.45845532417297363, -0.1114087924361229, 0.3647603988647461, 0.11585446447134018, -0.3294542133808136, 0.06639572978019714, 0.6805629134178162, 0.3249743580818176, 0.7836873531341553, 0.02258574590086937, -0.10404499620199203, 0.0770040899515152, 0.08576034754514694, 0.5699759721755981, -0.010376030579209328, -0.5029566287994385, -0.5079394578933716, -0.19580534100532532, 0.06726675480604172, 0.35176894068717957, -0.026191316545009613, 0.46152207255363464, 0.28769153356552124, 0.09594085812568665, 0.130301371216774, 0.2274671047925949, -0.33337897062301636, -0.24588961899280548, -0.515285849571228, 0.17248229682445526, -0.18501625955104828, 0.6305117011070251, -0.039075564593076706, 0.05472381412982941, -0.15533366799354553, 0.3331942558288574, -0.1436445713043213, -0.174470916390419, 0.47622838616371155, -0.24366562068462372, -0.23554518818855286, 0.1853944957256317, -0.035319436341524124, -0.11939332634210587, -0.4554460644721985, 0.12441790103912354, 0.212041437625885, 0.3181895613670349, -0.29627180099487305, 0.1851230263710022, -0.007240481674671173, -0.04598422348499298, 0.2696440517902374, -1.5808149576187134, 0.3737697899341583, -0.09111785888671875, -0.08035162091255188, -0.6728447675704956, 0.007139960303902626, 0.07938646525144577, 0.316256046295166, -0.16719278693199158, 0.25451064109802246, -0.06591041386127472, 0.1943005621433258, 0.12621650099754333, -0.2275625467300415, 0.4041595458984375, 0.25665801763534546]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'To be, or not to be, that is the question.',\n",
       " \"Budumba - this is a rare creature that inhabits the forests of the Northern Lands. It is named after the Native Americans who named it 'Budumba' because of its large head. It uses it's head to crack rocks and break up large trees to find food. It is a common sight in the forest.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset_path = \"my_dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n",
    "index_path = \"my_index.faiss\"    # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n",
    "dataset = Dataset.load_from_disk(dataset_path)\n",
    "print(dataset[0])\n",
    "retriever = CustomFAISSRetriever(index_path, dataset)\n",
    "retriever.retrieve(\"What is a common sight in the forest with the large head?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-7b-it\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XayEss\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\generation\\utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><CONTEXT>Budumba - this is a rare creature that inhabits the forests of the Northern Lands. It is named after the Native Americans who named it 'Budumba' because of its large head. It uses it's head to crack rocks and break up large trees to find food. It is a common sight in the forest.<QUESTION>How does Budumba - find it's food?\n",
      "\n",
      "The answer is: Budumba uses its head to crack rocks and break up large trees to find food.<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How does Budumba - find it's food?\"\n",
    "result = retriever.retrieve(input_text, k=3)\n",
    "input_text = \"<CONTEXT>\" + result[0] + \"<QUESTION>\" + input_text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fb2cfaa0d73e73f44b6b1563ac19fe822a7ee09c64df8d4dd703b9d43dc9a98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
